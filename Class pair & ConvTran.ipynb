{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf6af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7308c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import LeaveOneOut, RandomizedSearchCV, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed03ae",
   "metadata": {},
   "source": [
    "## Transform data on Minirocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8077cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30 second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(data_list)\n",
    "y = np.array(labels_list)\n",
    "patient_ids = np.array(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33006fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to perform minirocket\n",
    "X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf17075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the minirocket transformer\n",
    "minirocket = MiniRocket(random_state=123)\n",
    "X_transformed = minirocket.fit_transform(X_reshaped) # transform the entire dataset using minirocket\n",
    "\n",
    "# scale the transformed data\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_scaled = scaler.fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10767a",
   "metadata": {},
   "source": [
    "## Class pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89ed7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class pair (0, 1): F1 Score = 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.47      0.60        47\n",
      "           1       0.90      0.98      0.94       233\n",
      "\n",
      "    accuracy                           0.90       280\n",
      "   macro avg       0.87      0.73      0.77       280\n",
      "weighted avg       0.89      0.90      0.88       280\n",
      "\n",
      "Class pair (0, 2): F1 Score = 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71        47\n",
      "           1       0.79      0.87      0.83        69\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.78      0.76      0.77       116\n",
      "weighted avg       0.78      0.78      0.78       116\n",
      "\n",
      "Class pair (0, 3): F1 Score = 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.62      0.72        47\n",
      "           1       0.84      0.95      0.89        98\n",
      "\n",
      "    accuracy                           0.84       145\n",
      "   macro avg       0.85      0.78      0.80       145\n",
      "weighted avg       0.84      0.84      0.83       145\n",
      "\n",
      "Class pair (0, 4): F1 Score = 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.78        47\n",
      "           1       0.80      0.86      0.83        56\n",
      "\n",
      "    accuracy                           0.81       103\n",
      "   macro avg       0.81      0.80      0.80       103\n",
      "weighted avg       0.81      0.81      0.80       103\n",
      "\n",
      "Class pair (1, 2): F1 Score = 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       233\n",
      "           1       0.82      0.78      0.80        69\n",
      "\n",
      "    accuracy                           0.91       302\n",
      "   macro avg       0.88      0.87      0.87       302\n",
      "weighted avg       0.91      0.91      0.91       302\n",
      "\n",
      "Class pair (1, 3): F1 Score = 0.66\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       234\n",
      "           1       0.72      0.61      0.66        98\n",
      "\n",
      "    accuracy                           0.82       332\n",
      "   macro avg       0.79      0.76      0.77       332\n",
      "weighted avg       0.81      0.82      0.81       332\n",
      "\n",
      "Class pair (1, 4): F1 Score = 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       233\n",
      "           1       0.88      0.50      0.64        56\n",
      "\n",
      "    accuracy                           0.89       289\n",
      "   macro avg       0.88      0.74      0.79       289\n",
      "weighted avg       0.89      0.89      0.88       289\n",
      "\n",
      "Class pair (2, 3): F1 Score = 0.76\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.52      0.59        69\n",
      "           1       0.71      0.83      0.76        98\n",
      "\n",
      "    accuracy                           0.70       167\n",
      "   macro avg       0.69      0.67      0.68       167\n",
      "weighted avg       0.70      0.70      0.69       167\n",
      "\n",
      "Class pair (2, 4): F1 Score = 0.86\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90        69\n",
      "           1       0.92      0.80      0.86        56\n",
      "\n",
      "    accuracy                           0.88       125\n",
      "   macro avg       0.89      0.87      0.88       125\n",
      "weighted avg       0.88      0.88      0.88       125\n",
      "\n",
      "Class pair (3, 4): F1 Score = 0.88\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93        98\n",
      "           1       0.91      0.86      0.88        56\n",
      "\n",
      "    accuracy                           0.92       154\n",
      "   macro avg       0.91      0.90      0.91       154\n",
      "weighted avg       0.92      0.92      0.92       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X_scaled and y_encoded are already prepared (full dataset after MiniRocket transformation and scaling)\n",
    "\n",
    "# Define the classes\n",
    "classes = np.unique(y_encoded)\n",
    "\n",
    "# Store results for each pair\n",
    "pair_results = {}\n",
    "\n",
    "# Loop through all pairs of classes\n",
    "for class1, class2 in itertools.combinations(classes, 2):\n",
    "    # Filter the data for the two classes\n",
    "    mask = (y_encoded == class1) | (y_encoded == class2)\n",
    "    X_pair = X_scaled[mask]\n",
    "    y_pair = y_encoded[mask]\n",
    "\n",
    "    # Encode the two classes as binary\n",
    "    y_pair = LabelEncoder().fit_transform(y_pair)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pair, y_pair, test_size=0.3, random_state=2, stratify=y_pair)\n",
    "    \n",
    "    # Initialize the XGBoost classifier\n",
    "    xgb_classifier = XGBClassifier(random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    pair_results[(class1, class2)] = f1\n",
    "    \n",
    "    # Print results for each pair\n",
    "    print(f\"Class pair ({class1}, {class2}): F1 Score = {f1:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce47443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most separable pair: (0, 1) with F1 Score = 0.94\n"
     ]
    }
   ],
   "source": [
    "most_separable_pair = max(pair_results, key=pair_results.get)\n",
    "print(f\"\\nMost separable pair: {most_separable_pair} with F1 Score = {pair_results[most_separable_pair]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa2d15c",
   "metadata": {},
   "source": [
    "So most separable pair is AW and LA. It's hard to distinguish between LA to DA and LA to RE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4abdb2",
   "metadata": {},
   "source": [
    "## ConvTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6f77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc22692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30-second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32)  # Shape: (num_samples, sequence_length)\n",
    "y = torch.tensor(y_encoded, dtype=torch.long)  # Shape: (num_samples,)\n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "149ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from ConvTran.Models.AbsolutePositionalEncoding import tAPE, AbsolutePositionalEncoding, LearnablePositionalEncoding\n",
    "from ConvTran.Models.Attention import Attention, Attention_Rel_Scl, Attention_Rel_Vec\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "def model_factory(config):\n",
    "    if config['Net_Type'][0] == 'T':\n",
    "        model = Transformer(config, num_classes=config['num_labels'])\n",
    "    elif config['Net_Type'][0] == 'CC-T':\n",
    "        model = CasualConvTran(config, num_classes=config['num_labels'])\n",
    "    else:\n",
    "        model = ConvTran(config, num_classes=config['num_labels'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(\n",
    "            nn.Linear(channel_size, emb_size),\n",
    "            nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        )\n",
    "\n",
    "        self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        self.LayerNorm1 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        if self.Rel_pos_encode == 'Scalar':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_src = self.embed_layer(x.permute(0, 2, 1))\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src = self.Fix_Position(x_src)\n",
    "        att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm1(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        # out = out.permute(1, 0, 2)\n",
    "        # out = self.out(out[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(nn.Conv2d(1, emb_size*4, kernel_size=[1, 8], padding='same'),\n",
    "                                         nn.BatchNorm2d(emb_size*4),\n",
    "                                         nn.GELU())\n",
    "\n",
    "        self.embed_layer2 = nn.Sequential(nn.Conv2d(emb_size*4, emb_size, kernel_size=[channel_size, 1], padding='valid'),\n",
    "                                          nn.BatchNorm2d(emb_size),\n",
    "                                          nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = AbsolutePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CasualConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.causal_Conv1 = nn.Sequential(CausalConv1d(channel_size, emb_size, kernel_size=8, stride=2, dilation=1),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv2 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=5, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv3 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=3, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(CausalConv1d, self).forward(nn.functional.pad(x, (self.__padding, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4755442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdmnhtrang/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 59.15%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 67.87%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 64.89%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 37.66%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 46.33%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 46.37%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 68.83%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 34.88%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 39.62%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 32.14%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 63.64%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 19.05%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 32.98%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 53.85%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 68.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "    'emb_size': 32,  \n",
    "    'num_heads': 4,  \n",
    "    'dim_ff': 64,  \n",
    "    'dropout': 0.1,\n",
    "    'Fix_pos_encode': 'tAPE',\n",
    "    'Rel_pos_encode': 'Scalar',\n",
    "    'num_labels': 5\n",
    "}\n",
    "\n",
    "\n",
    "# Initialise the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the ConvTran model\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        accuracy = (all_preds == y_test).sum().item() / y_test.size(0)\n",
    "        print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Data_shape': (64, 1, 128),  # Example: (batch_size, channels, sequence_length)\n",
    "    'emb_size': 64,\n",
    "    'num_heads': 8,\n",
    "    'dim_ff': 128,\n",
    "    'dropout': 0.2,\n",
    "    'Fix_pos_encode': 'Learn',  # Can be 'Sin', 'Learn', or 'None'\n",
    "    'Rel_pos_encode': 'Scalar',  # Can be 'Scalar', 'Vector', or 'None'\n",
    "    'num_labels': 5,  # Number of classes\n",
    "    'Net_Type': 'C'  # 'T' for Transformer, 'CC-T' for CasualConvTran, 'C' for ConvTran\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd2c4b",
   "metadata": {},
   "source": [
    "'CC-T' for CasualConvTran, 'C' for ConvTran - these have higher accuracy then Transformer\n",
    "\n",
    "number of input channels = 1 since EEG data is usually single-channel per electrode\n",
    "30 seconds of EEG data at a sampling rate of 128 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c791c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153e2bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.5931\n",
      "Epoch [2/10], Loss: 1.4378\n",
      "Epoch [3/10], Loss: 1.3834\n",
      "Epoch [4/10], Loss: 1.3724\n",
      "Epoch [5/10], Loss: 1.3435\n",
      "Epoch [6/10], Loss: 1.3003\n",
      "Epoch [7/10], Loss: 1.2668\n",
      "Epoch [8/10], Loss: 1.2060\n",
      "Epoch [9/10], Loss: 1.1798\n",
      "Epoch [10/10], Loss: 1.1639\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example training data (replace with actual data)\n",
    "X_train = torch.randn(32, 1, 128)  # Example: (batch_size, channels, sequence_length)\n",
    "y_train = torch.randint(0, 5, (32,))  # Example: 5 classes\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(10):  # Example: 10 epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2953469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32)  # Shape: (num_samples, sequence_length)\n",
    "y = torch.tensor(y_encoded, dtype=torch.long)  # Shape: (num_samples,)\n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf187871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 3  # You can change this number based on your needs\n",
    "\n",
    "# # Define optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, y_train)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Print loss for every epoch\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eff002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "# Initialize the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Initialize your model (example using ConvTran)\n",
    "    model = ConvTran(config, num_classes=5)  # Replace with your actual config and number of classes\n",
    "    \n",
    "    # Train the model (you will need to implement the training loop)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "        print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
