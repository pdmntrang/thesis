{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf6af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7308c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut, RandomizedSearchCV, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed03ae",
   "metadata": {},
   "source": [
    "## Transform data on Minirocket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e754ca",
   "metadata": {},
   "source": [
    "#### 30 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8077cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30 second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(data_list)\n",
    "y = np.array(labels_list)\n",
    "patient_ids = np.array(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33006fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to perform minirocket\n",
    "X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80deb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
    "\n",
    "# MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
    "# Classification\n",
    "\n",
    "# https://arxiv.org/abs/2012.08791\n",
    "\n",
    "from numba import njit, prange, vectorize\n",
    "import numpy as np\n",
    "\n",
    "@njit(\"float32[:](float32[:,:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
    "def _fit_biases(X, dilations, num_features_per_dilation, quantiles):\n",
    "\n",
    "    num_examples, input_length = X.shape\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    biases = np.zeros(num_features, dtype = np.float32)\n",
    "\n",
    "    feature_index_start = 0\n",
    "\n",
    "    for dilation_index in range(num_dilations):\n",
    "\n",
    "        dilation = dilations[dilation_index]\n",
    "        padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "        for kernel_index in range(num_kernels):\n",
    "\n",
    "            feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "            _X = X[np.random.randint(num_examples)]\n",
    "\n",
    "            A = -_X          # A = alpha * X = -X\n",
    "            G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
    "                C_gamma[gamma_index, -end:] = G[:end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
    "                C_gamma[gamma_index, :-start] = G[start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "\n",
    "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
    "\n",
    "            feature_index_start = feature_index_end\n",
    "\n",
    "    return biases\n",
    "\n",
    "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    num_features_per_kernel = num_features // num_kernels\n",
    "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
    "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
    "\n",
    "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
    "    dilations, num_features_per_dilation = \\\n",
    "    np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
    "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
    "\n",
    "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
    "    i = 0\n",
    "    while remainder > 0:\n",
    "        num_features_per_dilation[i] += 1\n",
    "        remainder -= 1\n",
    "        i = (i + 1) % len(num_features_per_dilation)\n",
    "\n",
    "    return dilations, num_features_per_dilation\n",
    "\n",
    "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
    "def _quantiles(n):\n",
    "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
    "\n",
    "def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
    "\n",
    "    _, input_length = X.shape\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
    "\n",
    "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
    "\n",
    "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
    "\n",
    "    biases = _fit_biases(X, dilations, num_features_per_dilation, quantiles)\n",
    "\n",
    "    return dilations, num_features_per_dilation, biases\n",
    "\n",
    "# _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
    "@vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
    "def _PPV(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "@njit(\"float32[:,:](float32[:,:],Tuple((int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
    "def transform(X, parameters):\n",
    "\n",
    "    num_examples, input_length = X.shape\n",
    "\n",
    "    dilations, num_features_per_dilation, biases = parameters\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
    "\n",
    "    for example_index in prange(num_examples):\n",
    "\n",
    "        _X = X[example_index]\n",
    "\n",
    "        A = -_X          # A = alpha * X = -X\n",
    "        G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "        feature_index_start = 0\n",
    "\n",
    "        for dilation_index in range(num_dilations):\n",
    "\n",
    "            _padding0 = dilation_index % 2\n",
    "\n",
    "            dilation = dilations[dilation_index]\n",
    "            padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
    "                C_gamma[gamma_index, -end:] = G[:end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
    "                C_gamma[gamma_index, :-start] = G[start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            for kernel_index in range(num_kernels):\n",
    "\n",
    "                feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "                _padding1 = (_padding0 + kernel_index) % 2\n",
    "\n",
    "                index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "                C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "\n",
    "                if _padding1 == 0:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
    "                else:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
    "\n",
    "                feature_index_start = feature_index_end\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f2c9",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea245d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 73.24%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 49.32%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 60.64%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 32.47%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 58.72%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 51.96%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 55.19%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 16.28%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 45.28%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 15.18%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 45.45%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 21.43%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 34.04%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 46.15%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 45.83%\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialise variables to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient (Leave-One-Patient-Out)\n",
    "for train_idx, test_idx in logo.split(X_transformed, y_encoded, groups=patient_ids):\n",
    "    # Split the data based on patient IDs\n",
    "    X_train, X_test = X_transformed[train_idx], X_transformed[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "    # Initialise the XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Store the results\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "    \n",
    "    # Calculate accuracy for this patient\n",
    "    accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "    print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa9204c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 73.24%\n",
      "Overall F1 Score: 0.63\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Overall Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Overall F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924cf48",
   "metadata": {},
   "source": [
    "#### 2 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f860788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30 second windows\n",
    "    segment_size = 128 * 2\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(data_list)\n",
    "y = np.array(labels_list)\n",
    "patient_ids = np.array(patient_ids)\n",
    "\n",
    "# Reshape to perform minirocket\n",
    "X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907e1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 45.29%\n",
      "Overall F1 Score (Weighted): 0.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialise variables to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient (Leave-One-Patient-Out)\n",
    "for train_idx, test_idx in logo.split(X_transformed, y_encoded, groups=patient_ids):\n",
    "    # Split the data based on patient IDs\n",
    "    X_train, X_test = X_transformed[train_idx], X_transformed[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "    # Initialise the XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Store the results\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate F1 score (weighted)\n",
    "overall_f1_score = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'Overall F1 Score (Weighted): {overall_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9182ba",
   "metadata": {},
   "source": [
    "2 second window F1 and accuracy is lower than 30 second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a62de5",
   "metadata": {},
   "source": [
    "## other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf17075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialise the minirocket transformer\n",
    "# minirocket = MiniRocket(random_state=123)\n",
    "# X_transformed = minirocket.fit_transform(X_reshaped) # transform the entire dataset using minirocket\n",
    "\n",
    "# # scale the transformed data\n",
    "# scaler = StandardScaler(with_mean=False)\n",
    "# X_scaled = scaler.fit_transform(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312fd45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 64.79%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 50.23%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 61.70%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 40.26%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 55.96%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 51.96%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 63.64%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 25.58%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 50.94%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 24.11%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 47.27%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 19.05%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 32.98%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 47.86%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 47.92%\n"
     ]
    }
   ],
   "source": [
    "# # Encode the labels as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Initialize variables to store results\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "# all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "\n",
    "# for patient in all_patient_ids:\n",
    "#     # Split the data based on patient IDs\n",
    "#     X_train = X_scaled[patient_ids != patient]\n",
    "#     X_test = X_scaled[patient_ids == patient]\n",
    "#     y_train = y_encoded[patient_ids != patient]\n",
    "#     y_test = y_encoded[patient_ids == patient]\n",
    "    \n",
    "#     # Initialize the XGBoost classifier\n",
    "#     xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "#     # Train the classifier\n",
    "#     xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "#     # Predict on the test set\n",
    "#     y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "#     # Store the results\n",
    "#     all_preds.extend(y_pred)\n",
    "#     all_labels.extend(y_test)\n",
    "    \n",
    "#     # Calculate and print accuracy for the current patient\n",
    "#     accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "#     print(f'Patient ID: {patient}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4abdb2",
   "metadata": {},
   "source": [
    "## ConvTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c6f77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc22692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30-second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32) \n",
    "y = torch.tensor(y_encoded, dtype=torch.long)  # Shape: (num_samples,)\n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from ConvTran.Models.AbsolutePositionalEncoding import tAPE, AbsolutePositionalEncoding, LearnablePositionalEncoding\n",
    "from ConvTran.Models.Attention import Attention, Attention_Rel_Scl, Attention_Rel_Vec\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "def model_factory(config):\n",
    "    if config['Net_Type'][0] == 'T':\n",
    "        model = Transformer(config, num_classes=config['num_labels'])\n",
    "    elif config['Net_Type'][0] == 'CC-T':\n",
    "        model = CasualConvTran(config, num_classes=config['num_labels'])\n",
    "    else:\n",
    "        model = ConvTran(config, num_classes=config['num_labels'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(\n",
    "            nn.Linear(channel_size, emb_size),\n",
    "            nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        )\n",
    "\n",
    "        self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        self.LayerNorm1 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        if self.Rel_pos_encode == 'Scalar':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_src = self.embed_layer(x.permute(0, 2, 1))\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src = self.Fix_Position(x_src)\n",
    "        att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm1(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        # out = out.permute(1, 0, 2)\n",
    "        # out = self.out(out[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(nn.Conv2d(1, emb_size*4, kernel_size=[1, 8], padding='same'),\n",
    "                                         nn.BatchNorm2d(emb_size*4),\n",
    "                                         nn.GELU())\n",
    "\n",
    "        self.embed_layer2 = nn.Sequential(nn.Conv2d(emb_size*4, emb_size, kernel_size=[channel_size, 1], padding='valid'),\n",
    "                                          nn.BatchNorm2d(emb_size),\n",
    "                                          nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = AbsolutePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CasualConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.causal_Conv1 = nn.Sequential(CausalConv1d(channel_size, emb_size, kernel_size=8, stride=2, dilation=1),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv2 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=5, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv3 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=3, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(CausalConv1d, self).forward(nn.functional.pad(x, (self.__padding, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4755442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdmnhtrang/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 59.15%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 67.87%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 64.89%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 37.66%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 46.33%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 46.37%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 68.83%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 34.88%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 39.62%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 32.14%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 63.64%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 19.05%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 32.98%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 53.85%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 68.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "    'emb_size': 32,  \n",
    "    'num_heads': 4,  \n",
    "    'dim_ff': 64,  \n",
    "    'dropout': 0.1,\n",
    "    'Fix_pos_encode': 'tAPE',\n",
    "    'Rel_pos_encode': 'Scalar',\n",
    "    'num_labels': 5\n",
    "}\n",
    "\n",
    "\n",
    "# Initialise the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the ConvTran model\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        accuracy = (all_preds == y_test).sum().item() / y_test.size(0)\n",
    "        print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dc1fa",
   "metadata": {},
   "source": [
    "#### 2 sescond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9dc01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30-second windows\n",
    "    segment_size = 128 * 2\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32) \n",
    "y = torch.tensor(y_encoded, dtype=torch.long)  # Shape: (num_samples,)\n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f808eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdmnhtrang/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 47.55%\n",
      "Overall F1 Score (Weighted): 0.42\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "    'emb_size': 32,  \n",
    "    'num_heads': 4,  \n",
    "    'dim_ff': 64,  \n",
    "    'dropout': 0.1,\n",
    "    'Fix_pos_encode': 'tAPE',\n",
    "    'Rel_pos_encode': 'Scalar',\n",
    "    'num_labels': 5\n",
    "}\n",
    "\n",
    "# Initialise the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the ConvTran model\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_preds.append(predicted)\n",
    "        test_preds = torch.cat(test_preds)\n",
    "\n",
    "        # Collect predictions and true labels for overall metrics\n",
    "        all_preds.extend(test_preds.cpu().numpy())  # Convert to numpy\n",
    "        all_labels.extend(y_test.cpu().numpy())  # Convert to numpy\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate overall F1 score (weighted)\n",
    "overall_f1_score = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'Overall F1 Score (Weighted): {overall_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'Data_shape': (64, 1, 128),  # Example: (batch_size, channels, sequence_length)\n",
    "#     'emb_size': 64,\n",
    "#     'num_heads': 8,\n",
    "#     'dim_ff': 128,\n",
    "#     'dropout': 0.2,\n",
    "#     'Fix_pos_encode': 'Learn',  # Can be 'Sin', 'Learn', or 'None'\n",
    "#     'Rel_pos_encode': 'Scalar',  # Can be 'Scalar', 'Vector', or 'None'\n",
    "#     'num_labels': 5,  # Number of classes\n",
    "#     'Net_Type': 'C'  # 'T' for Transformer, 'CC-T' for CasualConvTran, 'C' for ConvTran\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd2c4b",
   "metadata": {},
   "source": [
    "'CC-T' for CasualConvTran, 'C' for ConvTran - these have higher accuracy then Transformer\n",
    "\n",
    "number of input channels = 1 since EEG data is usually single-channel per electrode\n",
    "30 seconds of EEG data at a sampling rate of 128 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdb680",
   "metadata": {},
   "source": [
    "## XGB vs ConvTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c72971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Patient ID  XGBoost Accuracy (%)  ConvTran Accuracy (%)\n",
      "0   L05200708_eeg_label.csv                 73.24                  59.15\n",
      "1   L05211742_eeg_label.csv                 49.32                  67.87\n",
      "2   L05250816_eeg_label.csv                 60.64                  64.89\n",
      "3   L05250921_eeg_label.csv                 32.47                  37.66\n",
      "4   L05271431_eeg_label.csv                 58.72                  46.33\n",
      "5   L05281010_eeg_label.csv                 51.96                  46.37\n",
      "6   L06101015_eeg_label.csv                 55.19                  68.83\n",
      "7   L06181302_eeg_label.csv                 16.28                  34.88\n",
      "8   L06181332_eeg_label.csv                 45.28                  39.62\n",
      "9   L06221009_eeg_label.csv                 15.18                  32.14\n",
      "10  L06221141_eeg_label.csv                 45.45                  63.64\n",
      "11  L06221219_eeg_label.csv                 21.43                  19.05\n",
      "12  L08181442_eeg_label.csv                 34.04                  32.98\n",
      "13  L08190811_eeg_label.csv                 46.15                  53.85\n",
      "14  L08190921_eeg_label.csv                 45.83                  68.06\n"
     ]
    }
   ],
   "source": [
    "xgb_patient_ids = [\n",
    "    'L05200708_eeg_label.csv', 'L05211742_eeg_label.csv', 'L05250816_eeg_label.csv', \n",
    "    'L05250921_eeg_label.csv', 'L05271431_eeg_label.csv', 'L05281010_eeg_label.csv', \n",
    "    'L06101015_eeg_label.csv', 'L06181302_eeg_label.csv', 'L06181332_eeg_label.csv', \n",
    "    'L06221009_eeg_label.csv', 'L06221141_eeg_label.csv', 'L06221219_eeg_label.csv', \n",
    "    'L08181442_eeg_label.csv', 'L08190811_eeg_label.csv', 'L08190921_eeg_label.csv'\n",
    "]\n",
    "\n",
    "xgb_accuracies = [\n",
    "    73.24, 49.32, 60.64, 32.47, 58.72, 51.96,\n",
    "    55.19, 16.28, 45.28, 15.18, 45.45, 21.43,\n",
    "    34.04, 46.15, 45.83\n",
    "]\n",
    "\n",
    "convtran_accuracies = [\n",
    "    59.15, 67.87, 64.89, 37.66, 46.33, 46.37, \n",
    "    68.83, 34.88, 39.62, 32.14, 63.64, 19.05, \n",
    "    32.98, 53.85, 68.06\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Patient ID': xgb_patient_ids,\n",
    "    'XGBoost Accuracy (%)': xgb_accuracies,\n",
    "    'ConvTran Accuracy (%)': convtran_accuracies\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "313fe9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mean accuracy: 43.41%\n",
      "ConvTran mean accuracy: 49.02%\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy_xgb = np.mean(xgb_accuracies)\n",
    "print(f'XGB mean accuracy: {mean_accuracy_xgb:.2f}%')\n",
    "\n",
    "mean_accuracy_ct = np.mean(convtran_accuracies)\n",
    "print(f'ConvTran mean accuracy: {mean_accuracy_ct:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
