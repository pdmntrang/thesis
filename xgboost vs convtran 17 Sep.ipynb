{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf6af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7308c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut, RandomizedSearchCV, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed03ae",
   "metadata": {},
   "source": [
    "## Transform data on Minirocket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e754ca",
   "metadata": {},
   "source": [
    "#### 30 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8077cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30 second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(data_list)\n",
    "y = np.array(labels_list)\n",
    "patient_ids = np.array(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33006fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to perform minirocket\n",
    "X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80deb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
    "\n",
    "# MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
    "# Classification\n",
    "\n",
    "# https://arxiv.org/abs/2012.08791\n",
    "\n",
    "from numba import njit, prange, vectorize\n",
    "import numpy as np\n",
    "\n",
    "@njit(\"float32[:](float32[:,:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
    "def _fit_biases(X, dilations, num_features_per_dilation, quantiles):\n",
    "\n",
    "    num_examples, input_length = X.shape\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    biases = np.zeros(num_features, dtype = np.float32)\n",
    "\n",
    "    feature_index_start = 0\n",
    "\n",
    "    for dilation_index in range(num_dilations):\n",
    "\n",
    "        dilation = dilations[dilation_index]\n",
    "        padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "        for kernel_index in range(num_kernels):\n",
    "\n",
    "            feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "            _X = X[np.random.randint(num_examples)]\n",
    "\n",
    "            A = -_X          # A = alpha * X = -X\n",
    "            G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
    "                C_gamma[gamma_index, -end:] = G[:end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
    "                C_gamma[gamma_index, :-start] = G[start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "\n",
    "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
    "\n",
    "            feature_index_start = feature_index_end\n",
    "\n",
    "    return biases\n",
    "\n",
    "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    num_features_per_kernel = num_features // num_kernels\n",
    "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
    "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
    "\n",
    "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
    "    dilations, num_features_per_dilation = \\\n",
    "    np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
    "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
    "\n",
    "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
    "    i = 0\n",
    "    while remainder > 0:\n",
    "        num_features_per_dilation[i] += 1\n",
    "        remainder -= 1\n",
    "        i = (i + 1) % len(num_features_per_dilation)\n",
    "\n",
    "    return dilations, num_features_per_dilation\n",
    "\n",
    "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
    "def _quantiles(n):\n",
    "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
    "\n",
    "def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
    "\n",
    "    _, input_length = X.shape\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
    "\n",
    "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
    "\n",
    "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
    "\n",
    "    biases = _fit_biases(X, dilations, num_features_per_dilation, quantiles)\n",
    "\n",
    "    return dilations, num_features_per_dilation, biases\n",
    "\n",
    "# _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
    "@vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
    "def _PPV(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "@njit(\"float32[:,:](float32[:,:],Tuple((int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
    "def transform(X, parameters):\n",
    "\n",
    "    num_examples, input_length = X.shape\n",
    "\n",
    "    dilations, num_features_per_dilation, biases = parameters\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
    "\n",
    "    for example_index in prange(num_examples):\n",
    "\n",
    "        _X = X[example_index]\n",
    "\n",
    "        A = -_X          # A = alpha * X = -X\n",
    "        G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "        feature_index_start = 0\n",
    "\n",
    "        for dilation_index in range(num_dilations):\n",
    "\n",
    "            _padding0 = dilation_index % 2\n",
    "\n",
    "            dilation = dilations[dilation_index]\n",
    "            padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "            C_alpha = np.zeros(input_length, dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[-end:] = C_alpha[-end:] + A[:end]\n",
    "                C_gamma[gamma_index, -end:] = G[:end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:-start] = C_alpha[:-start] + A[start:]\n",
    "                C_gamma[gamma_index, :-start] = G[start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            for kernel_index in range(num_kernels):\n",
    "\n",
    "                feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "                _padding1 = (_padding0 + kernel_index) % 2\n",
    "\n",
    "                index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "                C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "\n",
    "                if _padding1 == 0:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
    "                else:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
    "\n",
    "                feature_index_start = feature_index_end\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1f2c9",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bea245d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 73.24%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 49.32%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 60.64%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 32.47%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 58.72%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 51.96%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 55.19%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 16.28%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 45.28%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 15.18%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 45.45%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 21.43%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 34.04%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 46.15%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 45.83%\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialise variables to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient (Leave-One-Patient-Out)\n",
    "for train_idx, test_idx in logo.split(X_transformed, y_encoded, groups=patient_ids):\n",
    "    # Split the data based on patient IDs\n",
    "    X_train, X_test = X_transformed[train_idx], X_transformed[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "    # Initialise the XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Store the results\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "    \n",
    "    # Calculate accuracy for this patient\n",
    "    accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "    print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa9204c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 73.24%\n",
      "Overall F1 Score: 0.63\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Overall Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Overall F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c03893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-18 00:01:49,245] A new study created in memory with name: no-name-00c0a590-f234-46c9-9506-48fffd7b367d\n",
      "[I 2024-09-18 00:26:54,173] Trial 0 finished with value: 0.4643025347570714 and parameters: {'learning_rate': 0.1693489342438168, 'max_depth': 4, 'n_estimators': 256, 'subsample': 0.8265661936472543, 'colsample_bytree': 0.6150105512578169}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 01:01:27,843] Trial 1 finished with value: 0.4526451381676521 and parameters: {'learning_rate': 0.04853493756638328, 'max_depth': 6, 'n_estimators': 148, 'subsample': 0.9069294818744005, 'colsample_bytree': 0.6876017005020187}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 01:22:05,226] Trial 2 finished with value: 0.4465491947259446 and parameters: {'learning_rate': 0.2769729378226353, 'max_depth': 3, 'n_estimators': 265, 'subsample': 0.721927744764769, 'colsample_bytree': 0.8214973643761673}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 01:52:54,696] Trial 3 finished with value: 0.42466826118180023 and parameters: {'learning_rate': 0.2323725109282091, 'max_depth': 8, 'n_estimators': 264, 'subsample': 0.663311413565603, 'colsample_bytree': 0.8749833953244411}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 02:46:42,299] Trial 4 finished with value: 0.44853551559282323 and parameters: {'learning_rate': 0.037971565969501485, 'max_depth': 9, 'n_estimators': 132, 'subsample': 0.9902802839423139, 'colsample_bytree': 0.73512223844172}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 03:14:44,541] Trial 5 finished with value: 0.4365991553935913 and parameters: {'learning_rate': 0.2515228444744659, 'max_depth': 8, 'n_estimators': 273, 'subsample': 0.834876971486928, 'colsample_bytree': 0.7463890261447609}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 03:33:42,811] Trial 6 finished with value: 0.4339386292581842 and parameters: {'learning_rate': 0.2782116829325851, 'max_depth': 4, 'n_estimators': 217, 'subsample': 0.62856428747271, 'colsample_bytree': 0.6146870095107148}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 03:55:58,740] Trial 7 finished with value: 0.454358716247115 and parameters: {'learning_rate': 0.21539254768643362, 'max_depth': 5, 'n_estimators': 173, 'subsample': 0.8096450091134623, 'colsample_bytree': 0.9411376408178673}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 04:59:03,968] Trial 8 finished with value: 0.46139762649217836 and parameters: {'learning_rate': 0.07466559221304599, 'max_depth': 9, 'n_estimators': 278, 'subsample': 0.9385632612121354, 'colsample_bytree': 0.891561076521235}. Best is trial 0 with value: 0.4643025347570714.\n",
      "[I 2024-09-18 05:21:54,791] Trial 9 finished with value: 0.44290439651983504 and parameters: {'learning_rate': 0.23526417454303825, 'max_depth': 10, 'n_estimators': 135, 'subsample': 0.9157720730645975, 'colsample_bytree': 0.7375755470918659}. Best is trial 0 with value: 0.4643025347570714.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Study' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Output the best parameters\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Study' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest values for hyperparameters (with rounding)\n",
    "    learning_rate = round(trial.suggest_float('learning_rate', 0.01, 0.3), 2) \n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 300)\n",
    "    subsample = round(trial.suggest_float('subsample', 0.6, 1.0), 1)\n",
    "    colsample_bytree = round(trial.suggest_float('colsample_bytree', 0.6, 1.0), 1)\n",
    "\n",
    "    # Parameters for XGBoost\n",
    "    param = {\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': max_depth,\n",
    "        'n_estimators': n_estimators,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'random_state': 2\n",
    "    }\n",
    "\n",
    "    # Create the XGBoost classifier\n",
    "    xgb_classifier = XGBClassifier(**param)\n",
    "\n",
    "    # Perform cross-validation and return the average accuracy score (LOPO)\n",
    "    accuracy = cross_val_score(xgb_classifier, X_transformed, y_encoded, cv=logo, groups=patient_ids, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean accuracy over the cross-validation splits\n",
    "    return accuracy.mean()\n",
    "\n",
    "# Start the optimization process\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"Best parameters:\", study.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924cf48",
   "metadata": {},
   "source": [
    "#### 2 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f860788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30 second windows\n",
    "    segment_size = 128 * 2\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(data_list)\n",
    "y = np.array(labels_list)\n",
    "patient_ids = np.array(patient_ids)\n",
    "\n",
    "# Reshape to perform minirocket\n",
    "X_reshaped = X.reshape(X.shape[0], 1, X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa3b16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 56.84%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 46.49%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 50.39%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 44.39%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 47.97%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 47.75%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 48.38%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 32.77%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 35.74%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 29.47%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 44.74%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 24.10%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 34.46%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 51.05%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 54.00%\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialise variables to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient (Leave-One-Patient-Out)\n",
    "for train_idx, test_idx in logo.split(X_transformed, y_encoded, groups=patient_ids):\n",
    "    # Split the data based on patient IDs\n",
    "    X_train, X_test = X_transformed[train_idx], X_transformed[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "    # Initialise the XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Store the results\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "    \n",
    "    # Calculate accuracy for this patient\n",
    "    accuracy = (y_pred == y_test).sum() / len(y_test)\n",
    "    print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf11a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907e1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 45.29%\n",
      "Overall F1 Score (Weighted): 0.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Encode the labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initialise variables to store results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_patient_ids = np.unique(patient_ids)\n",
    "\n",
    "# Convert data types to match what the functions expect\n",
    "X_float32 = X.astype(np.float32)  # Ensure X is float32\n",
    "parameters = fit(X_float32)  \n",
    "\n",
    "# Transform the data\n",
    "X_transformed = transform(X_float32, parameters)\n",
    "\n",
    "# Initialise LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient (Leave-One-Patient-Out)\n",
    "for train_idx, test_idx in logo.split(X_transformed, y_encoded, groups=patient_ids):\n",
    "    # Split the data based on patient IDs\n",
    "    X_train, X_test = X_transformed[train_idx], X_transformed[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    \n",
    "    # Initialise the XGBoost classifier\n",
    "    xgb_classifier = xgb.XGBClassifier(eval_metric='mlogloss', random_state=2)\n",
    "    \n",
    "    # Train the classifier\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Store the results\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate F1 score (weighted)\n",
    "overall_f1_score = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'Overall F1 Score (Weighted): {overall_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9182ba",
   "metadata": {},
   "source": [
    "2 second window F1 and accuracy is lower than 30 second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4abdb2",
   "metadata": {},
   "source": [
    "## ConvTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6f77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149ce420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from ConvTran.Models.AbsolutePositionalEncoding import tAPE, AbsolutePositionalEncoding, LearnablePositionalEncoding\n",
    "from ConvTran.Models.Attention import Attention, Attention_Rel_Scl, Attention_Rel_Vec\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "def model_factory(config):\n",
    "    if config['Net_Type'][0] == 'T':\n",
    "        model = Transformer(config, num_classes=config['num_labels'])\n",
    "    elif config['Net_Type'][0] == 'CC-T':\n",
    "        model = CasualConvTran(config, num_classes=config['num_labels'])\n",
    "    else:\n",
    "        model = ConvTran(config, num_classes=config['num_labels'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(\n",
    "            nn.Linear(channel_size, emb_size),\n",
    "            nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        )\n",
    "\n",
    "        self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        self.LayerNorm1 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        if self.Rel_pos_encode == 'Scalar':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_src = self.embed_layer(x.permute(0, 2, 1))\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src = self.Fix_Position(x_src)\n",
    "        att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm1(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        # out = out.permute(1, 0, 2)\n",
    "        # out = self.out(out[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.embed_layer = nn.Sequential(nn.Conv2d(1, emb_size*4, kernel_size=[1, 8], padding='same'),\n",
    "                                         nn.BatchNorm2d(emb_size*4),\n",
    "                                         nn.GELU())\n",
    "\n",
    "        self.embed_layer2 = nn.Sequential(nn.Conv2d(emb_size*4, emb_size, kernel_size=[channel_size, 1], padding='valid'),\n",
    "                                          nn.BatchNorm2d(emb_size),\n",
    "                                          nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = AbsolutePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CasualConvTran(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # Parameters Initialization -----------------------------------------------\n",
    "        channel_size, seq_len = config['Data_shape'][1], config['Data_shape'][2]\n",
    "        emb_size = config['emb_size']\n",
    "        num_heads = config['num_heads']\n",
    "        dim_ff = config['dim_ff']\n",
    "        self.Fix_pos_encode = config['Fix_pos_encode']\n",
    "        self.Rel_pos_encode = config['Rel_pos_encode']\n",
    "        # Embedding Layer -----------------------------------------------------------\n",
    "        self.causal_Conv1 = nn.Sequential(CausalConv1d(channel_size, emb_size, kernel_size=8, stride=2, dilation=1),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv2 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=5, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv3 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=3, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        if self.Fix_pos_encode == 'tAPE':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif self.Fix_pos_encode == 'Sin':\n",
    "            self.Fix_Position = tAPE(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "        elif config['Fix_pos_encode'] == 'Learn':\n",
    "            self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=config['dropout'], max_len=seq_len)\n",
    "\n",
    "        if self.Rel_pos_encode == 'eRPE':\n",
    "            self.attention_layer = Attention_Rel_Scl(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        elif self.Rel_pos_encode == 'Vector':\n",
    "            self.attention_layer = Attention_Rel_Vec(emb_size, num_heads, seq_len, config['dropout'])\n",
    "        else:\n",
    "            self.attention_layer = Attention(emb_size, num_heads, config['dropout'])\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(config['dropout']))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.out = nn.Linear(emb_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_src = self.embed_layer(x)\n",
    "        x_src = self.embed_layer2(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        if self.Fix_pos_encode != 'None':\n",
    "            x_src_pos = self.Fix_Position(x_src)\n",
    "            att = x_src + self.attention_layer(x_src_pos)\n",
    "        else:\n",
    "            att = x_src + self.attention_layer(x_src)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(CausalConv1d, self).forward(nn.functional.pad(x, (self.__padding, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0f69345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(AbsolutePositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a matrix of [max_len, d_model] for positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Define the position vector, shape [max_len, 1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate div_term to apply to the sin/cos functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the array (0::2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].size(1)]\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input x\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc22692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30-second windows\n",
    "    segment_size = 128 * 30\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32) \n",
    "y = torch.tensor(y_encoded, dtype=torch.long) \n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4755442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdmnhtrang/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: L05200708_eeg_label.csv, Accuracy: 59.15%\n",
      "Patient ID: L05211742_eeg_label.csv, Accuracy: 67.87%\n",
      "Patient ID: L05250816_eeg_label.csv, Accuracy: 64.89%\n",
      "Patient ID: L05250921_eeg_label.csv, Accuracy: 37.66%\n",
      "Patient ID: L05271431_eeg_label.csv, Accuracy: 46.33%\n",
      "Patient ID: L05281010_eeg_label.csv, Accuracy: 46.37%\n",
      "Patient ID: L06101015_eeg_label.csv, Accuracy: 68.83%\n",
      "Patient ID: L06181302_eeg_label.csv, Accuracy: 34.88%\n",
      "Patient ID: L06181332_eeg_label.csv, Accuracy: 39.62%\n",
      "Patient ID: L06221009_eeg_label.csv, Accuracy: 32.14%\n",
      "Patient ID: L06221141_eeg_label.csv, Accuracy: 63.64%\n",
      "Patient ID: L06221219_eeg_label.csv, Accuracy: 19.05%\n",
      "Patient ID: L08181442_eeg_label.csv, Accuracy: 32.98%\n",
      "Patient ID: L08190811_eeg_label.csv, Accuracy: 53.85%\n",
      "Patient ID: L08190921_eeg_label.csv, Accuracy: 68.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "    'emb_size': 32,  \n",
    "    'num_heads': 4,  \n",
    "    'dim_ff': 64,  \n",
    "    'dropout': 0.1,\n",
    "    'Fix_pos_encode': 'tAPE',\n",
    "    'Rel_pos_encode': 'Scalar',\n",
    "    'num_labels': 5\n",
    "}\n",
    "\n",
    "\n",
    "# Initialise the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the ConvTran model\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.append(predicted)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        accuracy = (all_preds == y_test).sum().item() / y_test.size(0)\n",
    "        print(f'Patient ID: {patient_ids[test_idx[0]]}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dc1fa",
   "metadata": {},
   "source": [
    "#### 2 sescond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9dc01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_paths = glob.glob('eeg_label/*_eeg_label.csv')\n",
    "\n",
    "data_list = []\n",
    "labels_list = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    eeg_data = pd.read_csv(file_path)\n",
    "    patient_id = file_path.split('/')[-1]\n",
    "    \n",
    "    # Segment the data into non-overlapping 30-second windows\n",
    "    segment_size = 128 * 2\n",
    "    num_segments = len(eeg_data) // segment_size\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = start_idx + segment_size\n",
    "        segment = eeg_data['EEG'].iloc[start_idx:end_idx].values\n",
    "        label = eeg_data['Label'].iloc[start_idx:end_idx].mode()[0]\n",
    "        \n",
    "        data_list.append(segment)\n",
    "        labels_list.append(label)\n",
    "        patient_ids.append(patient_id)  # Repeat patient ID for each segment\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels_list)\n",
    "\n",
    "# Convert lists to numpy arrays and then to PyTorch tensors\n",
    "X = torch.tensor(np.array(data_list), dtype=torch.float32) \n",
    "y = torch.tensor(y_encoded, dtype=torch.long)  # Shape: (num_samples,)\n",
    "patient_ids = np.array(patient_ids)  # Keep patient IDs as numpy array for easier indexing\n",
    "\n",
    "# Reshape X to include a channel dimension\n",
    "X = X.unsqueeze(1)  # Shape: (num_samples, 1, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f808eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdmnhtrang/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 47.55%\n",
      "Overall F1 Score (Weighted): 0.42\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "# Model configuration\n",
    "config = {\n",
    "    'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "    'emb_size': 32,  \n",
    "    'num_heads': 4,  \n",
    "    'dim_ff': 64,  \n",
    "    'dropout': 0.1,\n",
    "    'Fix_pos_encode': 'tAPE',\n",
    "    'Rel_pos_encode': 'Scalar',\n",
    "    'num_labels': 5\n",
    "}\n",
    "\n",
    "# Initialise the LOPO cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# Loop over each patient\n",
    "for train_idx, test_idx in logo.split(X, y, groups=patient_ids):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch dataset\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the ConvTran model\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "\n",
    "    # Train the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = []\n",
    "        for batch_X, _ in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_preds.append(predicted)\n",
    "        test_preds = torch.cat(test_preds)\n",
    "\n",
    "        # Collect predictions and true labels for overall metrics\n",
    "        all_preds.extend(test_preds.cpu().numpy())  # Convert to numpy\n",
    "        all_labels.extend(y_test.cpu().numpy())  # Convert to numpy\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Overall Accuracy: {overall_accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate overall F1 score (weighted)\n",
    "overall_f1_score = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(f'Overall F1 Score (Weighted): {overall_f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'Data_shape': (64, 1, 128),  # Example: (batch_size, channels, sequence_length)\n",
    "#     'emb_size': 64,\n",
    "#     'num_heads': 8,\n",
    "#     'dim_ff': 128,\n",
    "#     'dropout': 0.2,\n",
    "#     'Fix_pos_encode': 'Learn',  # Can be 'Sin', 'Learn', or 'None'\n",
    "#     'Rel_pos_encode': 'Scalar',  # Can be 'Scalar', 'Vector', or 'None'\n",
    "#     'num_labels': 5,  # Number of classes\n",
    "#     'Net_Type': 'C'  # 'T' for Transformer, 'CC-T' for CasualConvTran, 'C' for ConvTran\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd2c4b",
   "metadata": {},
   "source": [
    "'CC-T' for CasualConvTran, 'C' for ConvTran - these have higher accuracy then Transformer\n",
    "\n",
    "number of input channels = 1 since EEG data is usually single-channel per electrode\n",
    "30 seconds of EEG data at a sampling rate of 128 Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdb680",
   "metadata": {},
   "source": [
    "## XGB vs ConvTran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c72971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Patient ID  XGBoost Accuracy (%)  ConvTran Accuracy (%)\n",
      "0   L05200708_eeg_label.csv                 73.24                  59.15\n",
      "1   L05211742_eeg_label.csv                 49.32                  67.87\n",
      "2   L05250816_eeg_label.csv                 60.64                  64.89\n",
      "3   L05250921_eeg_label.csv                 32.47                  37.66\n",
      "4   L05271431_eeg_label.csv                 58.72                  46.33\n",
      "5   L05281010_eeg_label.csv                 51.96                  46.37\n",
      "6   L06101015_eeg_label.csv                 55.19                  68.83\n",
      "7   L06181302_eeg_label.csv                 16.28                  34.88\n",
      "8   L06181332_eeg_label.csv                 45.28                  39.62\n",
      "9   L06221009_eeg_label.csv                 15.18                  32.14\n",
      "10  L06221141_eeg_label.csv                 45.45                  63.64\n",
      "11  L06221219_eeg_label.csv                 21.43                  19.05\n",
      "12  L08181442_eeg_label.csv                 34.04                  32.98\n",
      "13  L08190811_eeg_label.csv                 46.15                  53.85\n",
      "14  L08190921_eeg_label.csv                 45.83                  68.06\n"
     ]
    }
   ],
   "source": [
    "xgb_patient_ids = [\n",
    "    'L05200708_eeg_label.csv', 'L05211742_eeg_label.csv', 'L05250816_eeg_label.csv', \n",
    "    'L05250921_eeg_label.csv', 'L05271431_eeg_label.csv', 'L05281010_eeg_label.csv', \n",
    "    'L06101015_eeg_label.csv', 'L06181302_eeg_label.csv', 'L06181332_eeg_label.csv', \n",
    "    'L06221009_eeg_label.csv', 'L06221141_eeg_label.csv', 'L06221219_eeg_label.csv', \n",
    "    'L08181442_eeg_label.csv', 'L08190811_eeg_label.csv', 'L08190921_eeg_label.csv'\n",
    "]\n",
    "\n",
    "xgb_accuracies = [\n",
    "    73.24, 49.32, 60.64, 32.47, 58.72, 51.96,\n",
    "    55.19, 16.28, 45.28, 15.18, 45.45, 21.43,\n",
    "    34.04, 46.15, 45.83\n",
    "]\n",
    "\n",
    "convtran_accuracies = [\n",
    "    59.15, 67.87, 64.89, 37.66, 46.33, 46.37, \n",
    "    68.83, 34.88, 39.62, 32.14, 63.64, 19.05, \n",
    "    32.98, 53.85, 68.06\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Patient ID': xgb_patient_ids,\n",
    "    'XGBoost Accuracy (%)': xgb_accuracies,\n",
    "    'ConvTran Accuracy (%)': convtran_accuracies\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "313fe9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mean accuracy: 43.41%\n",
      "ConvTran mean accuracy: 49.02%\n"
     ]
    }
   ],
   "source": [
    "mean_accuracy_xgb = np.mean(xgb_accuracies)\n",
    "print(f'XGB mean accuracy: {mean_accuracy_xgb:.2f}%')\n",
    "\n",
    "mean_accuracy_ct = np.mean(convtran_accuracies)\n",
    "print(f'ConvTran mean accuracy: {mean_accuracy_ct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32363363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 00:39:03,579] A new study created in memory with name: no-name-67ed364c-d44d-4acc-b845-06483091619d\n",
      "/home/dpha0015/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:454: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "[I 2024-09-17 01:42:37,808] Trial 0 finished with value: 0.5100581321808303 and parameters: {'emb_size': 40, 'num_heads': 2, 'dim_ff': 64, 'dropout': 0.4955238853208461}. Best is trial 0 with value: 0.5100581321808303.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Parameters: emb_size=40, num_heads=2, dim_ff=64, dropout=0.4955238853208461\n",
      "Average Accuracy: 51.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-17 02:40:31,591] Trial 1 finished with value: 0.5119252947807207 and parameters: {'emb_size': 16, 'num_heads': 2, 'dim_ff': 128, 'dropout': 0.26148163976167516}. Best is trial 1 with value: 0.5119252947807207.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Parameters: emb_size=16, num_heads=2, dim_ff=128, dropout=0.26148163976167516\n",
      "Average Accuracy: 51.19%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def convtran_objective(trial):\n",
    "    # Suggest hyperparameters for ConvTran\n",
    "    emb_size = trial.suggest_int('emb_size', 16, 64, step=8)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 4)\n",
    "    dim_ff = trial.suggest_int('dim_ff', 64, 256, step=64)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\n",
    "    # Model configuration\n",
    "    config = {\n",
    "        'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "        'emb_size': emb_size,\n",
    "        'num_heads': num_heads,\n",
    "        'dim_ff': dim_ff,\n",
    "        'dropout': dropout,\n",
    "        'Fix_pos_encode': 'tAPE',\n",
    "        'Rel_pos_encode': 'Scalar',\n",
    "        'num_labels': 5\n",
    "    }\n",
    "\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # GroupKFold for cross-validation\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X, y, groups=patient_ids):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for epoch in range(10):\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    # Average accuracy\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    # Log the hyperparameters and accuracy\n",
    "    print(f'Trial Parameters: emb_size={emb_size}, num_heads={num_heads}, dim_ff={dim_ff}, dropout={dropout}')\n",
    "    print(f'Average Accuracy: {avg_accuracy * 100:.2f}%')\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(convtran_objective, n_trials=20)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Parameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def convtran_objective(trial):\n",
    "    # Suggest hyperparameters for ConvTran\n",
    "    emb_size = trial.suggest_int('emb_size', 16, 64, step=8)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 4)\n",
    "    dim_ff = trial.suggest_int('dim_ff', 64, 256, step=64)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\n",
    "    # Model configuration\n",
    "    config = {\n",
    "        'Data_shape': (X.shape[0], 1, X.shape[2]),\n",
    "        'emb_size': emb_size,\n",
    "        'num_heads': num_heads,\n",
    "        'dim_ff': dim_ff,\n",
    "        'dropout': dropout,\n",
    "        'Fix_pos_encode': 'tAPE',\n",
    "        'Rel_pos_encode': 'Scalar',\n",
    "        'num_labels': 5\n",
    "    }\n",
    "\n",
    "    model = ConvTran(config, num_classes=config['num_labels'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # GroupKFold for cross-validation\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    accuracies = []\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X, y, groups=patient_ids):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for epoch in range(10):\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    # Average accuracy\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    # Log the hyperparameters and accuracy\n",
    "    print(f'Trial Parameters: emb_size={emb_size}, num_heads={num_heads}, dim_ff={dim_ff}, dropout={dropout}')\n",
    "    print(f'Average Accuracy: {avg_accuracy * 100:.2f}%')\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(convtran_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398dd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure emb_size is defined before the assertion\n",
    "emb_size = 32  # or any other appropriate value\n",
    "num_heads = 4  # Set this to match the attention heads you are using in your model\n",
    "\n",
    "# Ensure seq_len * emb_size is divisible by num_heads\n",
    "seq_len = X.shape[2]\n",
    "assert (seq_len * emb_size) % num_heads == 0, \"Incompatible shape for attention!\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
